{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "distributed-visit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 249 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2021.3.17-cp38-cp38-manylinux2014_x86_64.whl (737 kB)\n",
      "\u001b[K     |████████████████████████████████| 737 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 552 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=70c754b780e9fe3890504c4de83f533744717c1d3fd241d33ac258e28bef0839\n",
      "  Stored in directory: /home/vishwanath/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, joblib, regex, tqdm, nltk\n",
      "Successfully installed click-7.1.2 joblib-1.0.1 nltk-3.5 regex-2021.3.17 tqdm-4.59.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "reflected-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "conceptual-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vishwanath/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') #Useful for the stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "static-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/vishwanath/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') #Usefule for the lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-brass",
   "metadata": {},
   "source": [
    "**Question 1.** Write a python program to find out the words after '@' from the below sentences with the use of regex.\n",
    "\n",
    "\"xyz@gmail.com\",\n",
    "\"abc@yahoo.com\",\n",
    "\"xyz@hotmail.com\",\n",
    "\"abc@ineuron.ai\",\n",
    "\"xyz@outlook.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ranking-upset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gmail.com\n",
      "yahoo.com\n",
      "hotmail.com\n",
      "ineuron.ai\n",
      "outlook.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "my_list = [\"xyz@gmail.com\", \"abc@yahoo.com\", \"xyz@hotmail.com\", \"abc@ineuron.ai\", \"xyz@outlook.com\"]\n",
    "for email in my_list:\n",
    "    match = re.search('(?<=@)(.+)', email)\n",
    "    print(match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-gregory",
   "metadata": {},
   "source": [
    "**Question 2.** Write a python program with the use of regex to take out the word \"New\" from the following sentence.\n",
    "\n",
    "[\"New Delhi is the capital of India\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unsigned-interview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Delhi is the capital of India']\n"
     ]
    }
   ],
   "source": [
    "my_list = [\"New Delhi is the capital of India\"]\n",
    "for each in my_list:\n",
    "    op = re.sub(r'\\bNew\\b\\s+',\"\",each)            \n",
    "    print([op])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-adoption",
   "metadata": {},
   "source": [
    "**Question 3.** Create one python program in which you have to lowercase the sentence first and than delete digits from the following sentence.\n",
    "\n",
    "\"In India, 184 people got affected with Corona virus and 4 are died.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "independent-calibration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in india,  people got affected with corona virus and  are died.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"In India, 184 people got affected with Corona virus and 4 are died.\"\n",
    "lower_str = sent.lower()\n",
    "result = ''.join([i for i in sent.lower() if not i.isdigit()])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-percentage",
   "metadata": {},
   "source": [
    "**Question 4.** Do stemming, lemmatization and tokenization from the following sentence.\n",
    "\n",
    "\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fatal-holocaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hop that , when i hav built up my sav , i wil be abl to travel to hawa .\n",
      "I hope that , when I have built up my save , I will be abl to travel to hawai .\n",
      "i hope that , when i have built up my save , i will be abl to travel to hawai .\n",
      "I hope that , when I have built up my saving , I will be able to travel to Hawai .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, SnowballStemmer\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "Snowball = SnowballStemmer(\"english\")\n",
    "sent = \"I hope that, when I have built up my savings, I will be able to travel to Hawai.\"\n",
    "token = list(nltk.word_tokenize(sent))\n",
    "token\n",
    "for stemmer in (lancaster,porter,Snowball):\n",
    "    stemm = [stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stemm))\n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "lemm = [lemma.lemmatize(t) for t in token]\n",
    "print(\" \".join(lemm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-advantage",
   "metadata": {},
   "source": [
    "**Question 5.** Create one python program from the following sentence.\n",
    "\n",
    "\"I love NLP, not you\"\n",
    "\n",
    "output : ['I', 'l', 'N', 'n', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "burning-married",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'l', 'N', 'n', 'y']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = \"I love NLP, not you\"\n",
    "output = [word[0] for word in my_str.split()]\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
